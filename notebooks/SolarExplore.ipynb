{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import json\n",
    "import sys \n",
    "import os \n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import skimage.io\n",
    "import skimage.draw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_dir: str,\n",
    "                 annotation_dir: str,\n",
    "                 transforms:Optional[Callable]= None,\n",
    "                 mode: str = \"train\",\n",
    "                 val_size: float = 0.2) -> None:\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.annatation_dir = annotation_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.val_size = val_size\n",
    "    \n",
    "        with open(annotation_dir) as f:\n",
    "            annotations_dict = json.load(f)\n",
    "            \n",
    "        self.image_infos = list(annotations_dict.values())\n",
    "        self.image_ids = list(annotations_dict.keys())\n",
    "\n",
    "        # Optional: Split into train/val here\n",
    "        if mode in [\"train\", \"val\"]:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            train_ids, val_ids = train_test_split(self.image_ids, test_size=val_size, random_state=99)\n",
    "            self.image_ids = train_ids if mode == \"train\" else val_ids\n",
    "            self.image_infos = [annotations_dict[k] for k in self.image_ids]\n",
    "\n",
    "        # Filter out images with no regions\n",
    "        self.image_infos = [info for info in self.image_infos if info.get(\"regions\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        info = self.image_infos[idx]\n",
    "        filename = info[\"filename\"]\n",
    "        image_path = self._resolve_image_path(info)\n",
    "\n",
    "        #image = skimage.io.imread(image_path)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "            \n",
    "        if image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        masks, class_ids, boxes = self._generate_instance_masks(info, height, width)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": class_ids,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        # Albumentations expects numpy inputs\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                masks=masks.numpy(),\n",
    "                bboxes=boxes.tolist(),\n",
    "                labels=class_ids.tolist()\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            masks = torch.stack([torch.tensor(m) for m in transformed[\"masks\"]])\n",
    "            boxes = torch.tensor(transformed[\"bboxes\"])\n",
    "            class_ids = torch.tensor(transformed[\"labels\"])\n",
    "\n",
    "            target.update({\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": class_ids,\n",
    "                \"masks\": masks,\n",
    "            })\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0   # BE SURE WHEN TO CONVERT TO TENSOR\n",
    "        #print(image.shape)\n",
    "        return image, target\n",
    "        \n",
    "    def _resolve_image_path(self, info):\n",
    "        category = info.get(\"filename\", \"\").split()[0]  # fallback if category is in path eg ~/Clean/Clean (2).jpg\n",
    "        return os.path.join(self.dataset_dir, category, info[\"filename\"])\n",
    "        \n",
    "    def _generate_instance_masks(self, info, height, width):\n",
    "        polygons = info[\"regions\"]\n",
    "        num_instances = len(polygons)\n",
    "\n",
    "        mask = np.zeros((height, width, num_instances), dtype=np.uint8)\n",
    "        class_ids = []\n",
    "        boxes = []\n",
    "\n",
    "        for i, region in enumerate(polygons):\n",
    "            shape = region[\"shape_attributes\"]\n",
    "            attrs = region.get(\"region_attributes\", {})\n",
    "            shape_type = shape.get(\"name\")\n",
    "            class_name = attrs.get(\"class\", \"Clean\").lower()   # the key here is type not class \n",
    "            class_id = self._map_class_name(class_name)\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "            \n",
    "            ## Check If I need to add another mask shape\n",
    "            # Create mask\n",
    "            if shape_type == \"polygon\":\n",
    "                x = shape.get(\"all_points_x\")\n",
    "                y = shape.get(\"all_points_y\")\n",
    "                rr, cc = skimage.draw.polygon(y, x)\n",
    "            elif shape_type == \"rect\":\n",
    "                x, y = shape[\"x\"], shape[\"y\"]\n",
    "                h, w = shape[\"height\"], shape[\"width\"]\n",
    "                rr, cc = skimage.draw.rectangle(start=(y, x), extent=(h, w))   # row and column coordinates\n",
    "            elif shape_type == \"polyline\":\n",
    "                x = shape.get('all_points_x')\n",
    "                y = shape.get('all_points_y')\n",
    "                rr, cc = skimage.draw.polygon(y,x)\n",
    "            else:\n",
    "                continue  # skip unsupported types\n",
    "\n",
    "            rr = np.clip(np.round(rr).astype(int), 0, height - 1)\n",
    "            cc = np.clip(np.round(cc).astype(int), 0, width - 1)\n",
    "            mask[rr, cc, i] = 1   # highlight the region of interest\n",
    "\n",
    "            pos = np.where(mask[:, :, i])\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        masks = torch.tensor(mask.transpose(2, 0, 1), dtype=torch.uint8)\n",
    "        class_ids = torch.tensor(class_ids, dtype=torch.int64)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        return masks, class_ids, boxes\n",
    "    \n",
    "    \n",
    "    def _map_class_name(self, name):\n",
    "        class_map = {\n",
    "            \"clean\": 1,\n",
    "            \"dust\": 2,\n",
    "            \"physical\": 3,\n",
    "            \"electrical\": 4,\n",
    "            \"bird\": 5,\n",
    "            \"snow\": 6\n",
    "        }\n",
    "        return class_map.get(name.lower(), 1)\n",
    "    \n",
    "    \n",
    "    def _get_albumentations_transforms(train=True):\n",
    "        if train:\n",
    "            return A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "                A.Rotate(limit=15, p=0.5),\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            mask_params=A.MaskParams())\n",
    "        else:\n",
    "            return A.Compose([ToTensorV2()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DATA_DIR = 'c:/Users/panos/CVision/Data'\n",
    "COCO_WEIGHTS_PATH = 'c:/Users/panos/CVision/external/Mask_RCNN/mask_rcnn_coco.h5'\n",
    "# This is the path to your main VIA JSON annotation file\n",
    "ANNOTATION_JSON_PATH = 'c:/Users/panos/CVision/Data/via_project_10Jul2025_15h51m_json.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarConfig():\n",
    "    \"\"\"Configuration for training on MS COCO.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the COCO dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"solar\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Uncomment to train on 8 GPUs (default is 1)\n",
    "    # GPU_COUNT = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 6  # Background + Solar Panel Damage Categories\n",
    "\n",
    "    # Numbe of training steps pre epoch\n",
    "    STEPS_PER_EPOCH = 10\n",
    "\n",
    "    # Skip detection with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "\n",
    "    USE_MINI_MASK = False\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for key, val in self.to_dict().items():\n",
    "            print(f\"{key:30} {val}\")\n",
    "        # for a in dir(self):\n",
    "        #     if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "        #         print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SolarConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")   # Here we load the weights\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "    \n",
    "def evaluate(model, dataset_val, device):\n",
    "    model.train()\n",
    "    data_loader = DataLoader(dataset_val,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            # print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(data_loader)\n",
    "    return avg_val_loss\n",
    "    \n",
    "\n",
    "max_lr = 6e-3 \n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10    \n",
    "num_epochs = 30 \n",
    " \n",
    "def get_lr(it):\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < warmup_steps:\n",
    "            return max_lr * (it+1) / (warmup_steps+1)\n",
    "        # 2) in between, use cosine decay down to min learning rate\n",
    "        #decay_ratio = (it - warmup_steps) / (num_epochs - warmup_steps)\n",
    "        # Clamp decay_ratio to [0, 1] to prevent assertion errors in case of misaligned inputs\n",
    "        decay_ratio = min(1.0, max(0.0, (it - warmup_steps) / (num_epochs - warmup_steps)))\n",
    "        assert 0 <= decay_ratio <= 1 \n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))   # coeff starts at 1 and goes to 0\n",
    "        import matplotlib.pyplot as plt\n",
    "        lrs = [get_lr(i) for i in range(num_epochs)]\n",
    "        plt.plot(lrs); plt.title(\"Learning Rate Schedule\"); plt.show()\n",
    "        return min_lr + coeff * (max_lr - min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataset_train, dataset_val, device):\n",
    "    data_loader = DataLoader(dataset_train,\n",
    "                             batch_size=2,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False   # Freeze backbone\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        best_avg_val_loss = float('inf')\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            # print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            print(losses)\n",
    "            for param_group in optimizer.param_groups: \n",
    "                param_group['lr'] = get_lr(epoch)\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += losses.item()\n",
    "            avg_train_loss = running_loss / len(data_loader)\n",
    "            avg_val_loss = evaluate(model, dataset_val, device)\n",
    "            \n",
    "            # Save the best model \n",
    "            if avg_val_loss < best_avg_val_loss:\n",
    "                best_avg_val_loss = avg_train_loss\n",
    "                checkpoint_path = os.path.join(args.logs, f\"best_model_{epoch}.pth\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Saved best model at epoch {epoch+1} with val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss.item():.4f}, Val Loss: {avg_val_loss.item():.4f}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_splash(image, mask):\n",
    "    # If no masks detected, return grayscale image\n",
    "    if mask.size == 0:\n",
    "        gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n",
    "        return gray.astype(np.uint8)\n",
    "\n",
    "    # Collapse all masks into one\n",
    "    mask = (np.sum(mask, axis=0, keepdims=True) >= 1)  # [1, H, W]\n",
    "    mask = mask.transpose(1, 2, 0)  # [H, W, 1]\n",
    "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n",
    "    splash = np.where(mask, image, gray).astype(np.uint8)\n",
    "    return splash\n",
    "\n",
    "\n",
    "def draw_boxes_on_splash(splash_image, output, threshold=0.8, class_names=None):\n",
    "    keep = output['scores'] > threshold\n",
    "    boxes = output['boxes'][keep].cpu().numpy()\n",
    "    labels = output['labels'][keep].cpu().numpy()\n",
    "    scores = output['scores'][keep].cpu().numpy()\n",
    "\n",
    "    image_draw = splash_image.copy()\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box.astype(int)            \n",
    "        label_text = f\"{class_names[label] if class_names else label}: {score:.2f}\"\n",
    "        cv2.rectangle(image_draw, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "        cv2.putText(image_draw, label_text, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), thickness=1)\n",
    "\n",
    "    return image_draw\n",
    "\n",
    "def detect_and_color_splash_pytorch(model, image_path, device, threshold=0.8):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    image_tensor = transform(image).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model([image_tensor])[0]\n",
    "\n",
    "    # Filter out low-confidence detections\n",
    "    keep = output['scores'] > threshold\n",
    "    masks = output['masks'][keep].squeeze(1)  # [N, H, W]\n",
    "\n",
    "    # Convert image to numpy array\n",
    "    image_np = np.array(image)\n",
    "    print(f\"Detections: {len(output['scores'])}, Above threshold: {keep.sum().item()}\")\n",
    "    # Create color splash\n",
    "    splash = color_splash(image_np, masks.cpu().numpy())\n",
    "    final_image = draw_boxes_on_splash(splash, output, threshold)\n",
    "\n",
    "    file_name = \"/content/drive/MyDrive/CVision/splash_with_boxes_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n",
    "    cv2.imwrite(file_name, cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))\n",
    "    print(\"Saved with boxes and splash:\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 1 + 6  # background + 6 solar damage classes\n",
    "model = get_model(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "sys.argv = ['script.py', '--weights', 'C:/Users/panos/CVision/external/Mask_RCNN/mask_rcnn_coco.h5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse command line arguments\n",
    "parser = argparse.ArgumentParser(\n",
    "description='Train Mask R-CNN to detect Solar Panels Damages'\n",
    ")\n",
    "\n",
    "parser.add_argument('--mode',   # convert it to a positional argument in the .py file \n",
    "                    help='train or inference',\n",
    "                    required=False,\n",
    "                    default='train'\n",
    "                    )\n",
    "\n",
    "parser.add_argument('--dataset',\n",
    "                    required=False,\n",
    "                    metavar=IMAGE_DATA_DIR,\n",
    "                    help='Root directory of our dataset',\n",
    "                    default=IMAGE_DATA_DIR\n",
    "                    )\n",
    "### THE FOLLOWING IS SHOULD BE COMMEND OUT ###  \n",
    "parser.add_argument('--weights',\n",
    "                    required=False,\n",
    "                    help='Path to weights .pth file or \"coco\" ',\n",
    "                    default=''\n",
    "                    )\n",
    "\n",
    "parser.add_argument('--logs',\n",
    "                    required=False,\n",
    "                    metavar=r'C:\\Users\\panos\\CVision\\Logs',\n",
    "                    help='Path to logs and checkpoints',\n",
    "                    default='C:/Users/panos/CVision/Logs'\n",
    "                    )\n",
    "\n",
    "parser.add_argument('--image', required=False,\n",
    "                        metavar=\"path or URL to image\",\n",
    "                        help='Image to apply the color splash effect on')\n",
    "\n",
    "args = parser.parse_args()   # parser.parse_args(['--dataset', 'pass the path that the dataset is located']), alternative way to preset the value of the argument or we could use default \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  C:/Users/panos/CVision/external/Mask_RCNN/mask_rcnn_coco.h5\n",
      "Dataset:  c:/Users/panos/CVision/Data\n",
      "Logs:  C:/Users/panos/CVision/Logs\n"
     ]
    }
   ],
   "source": [
    "# Validate arguments\n",
    "if args.mode == \"train\":\n",
    "    assert args.dataset, \"Argument --dataset is required for training\"\n",
    "elif args.mode == \"splash\":\n",
    "    assert args.image or args.video,\\\n",
    "            \"Provide --image or --video to apply color splash\"\n",
    "\n",
    "print(\"Weights: \", args.weights)\n",
    "print(\"Dataset: \", args.dataset)\n",
    "print(\"Logs: \", args.logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SolarDataset(dataset_dir=IMAGE_DATA_DIR, annotation_dir=ANNOTATION_JSON_PATH, mode=\"train\", val_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2048, 1536])\n",
      "torch.Size([3, 585, 780])\n",
      "{'loss_classifier': tensor(1.7248, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1095, grad_fn=<DivBackward0>), 'loss_mask': tensor(2.9268, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.1498, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0147, grad_fn=<DivBackward0>)}\n",
      "torch.Size([3, 494, 736])\n",
      "{'loss_classifier': tensor(0.3028), 'loss_box_reg': tensor(0.0645), 'loss_mask': tensor(1.9568), 'loss_objectness': tensor(0.0220), 'loss_rpn_box_reg': tensor(0.0156)}\n",
      "torch.Size([3, 416, 624])\n",
      "{'loss_classifier': tensor(0.3010), 'loss_box_reg': tensor(0.1732), 'loss_mask': tensor(1.2524), 'loss_objectness': tensor(0.0002), 'loss_rpn_box_reg': tensor(0.0143)}\n",
      "torch.Size([3, 313, 500])\n",
      "{'loss_classifier': tensor(0.1137), 'loss_box_reg': tensor(0.0222), 'loss_mask': tensor(1.1638), 'loss_objectness': tensor(0.1070), 'loss_rpn_box_reg': tensor(0.0203)}\n",
      "torch.Size([3, 768, 768])\n",
      "{'loss_classifier': tensor(0.6536), 'loss_box_reg': tensor(0.2100), 'loss_mask': tensor(1.3543), 'loss_objectness': tensor(0.0752), 'loss_rpn_box_reg': tensor(0.0302)}\n",
      "torch.Size([3, 682, 1024])\n",
      "{'loss_classifier': tensor(0.2280), 'loss_box_reg': tensor(0.1242), 'loss_mask': tensor(1.5429), 'loss_objectness': tensor(0.0067), 'loss_rpn_box_reg': tensor(0.0025)}\n",
      "torch.Size([3, 391, 500])\n",
      "{'loss_classifier': tensor(0.8159), 'loss_box_reg': tensor(0.2565), 'loss_mask': tensor(1.4873), 'loss_objectness': tensor(0.6260), 'loss_rpn_box_reg': tensor(0.0525)}\n",
      "torch.Size([3, 323, 576])\n",
      "{'loss_classifier': tensor(0.1535), 'loss_box_reg': tensor(0.0862), 'loss_mask': tensor(1.4245), 'loss_objectness': tensor(0.0049), 'loss_rpn_box_reg': tensor(0.0026)}\n",
      "torch.Size([3, 1024, 768])\n",
      "{'loss_classifier': tensor(0.3782), 'loss_box_reg': tensor(0.1399), 'loss_mask': tensor(1.8708), 'loss_objectness': tensor(0.0331), 'loss_rpn_box_reg': tensor(0.0124)}\n",
      "torch.Size([3, 1200, 1920])\n",
      "{'loss_classifier': tensor(0.1545), 'loss_box_reg': tensor(0.0544), 'loss_mask': tensor(1.6770), 'loss_objectness': tensor(0.0106), 'loss_rpn_box_reg': tensor(0.0015)}\n",
      "torch.Size([3, 416, 740])\n",
      "{'loss_classifier': tensor(0.1787), 'loss_box_reg': tensor(0.0592), 'loss_mask': tensor(2.3646), 'loss_objectness': tensor(0.0232), 'loss_rpn_box_reg': tensor(0.0105)}\n",
      "Saved best model at epoch 1 with val loss: 2.1636\n",
      "torch.Size([3, 1836, 2411])\n",
      "torch.Size([3, 480, 640])\n",
      "{'loss_classifier': tensor(0.4600, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1683, grad_fn=<DivBackward0>), 'loss_mask': tensor(1.9502, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.0779, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0094, grad_fn=<DivBackward0>)}\n",
      "torch.Size([3, 494, 736])\n",
      "{'loss_classifier': tensor(0.5105), 'loss_box_reg': tensor(0.1220), 'loss_mask': tensor(0.7103), 'loss_objectness': tensor(0.0092), 'loss_rpn_box_reg': tensor(0.0131)}\n",
      "torch.Size([3, 416, 624])\n",
      "{'loss_classifier': tensor(0.2722), 'loss_box_reg': tensor(0.1339), 'loss_mask': tensor(0.7335), 'loss_objectness': tensor(0.0176), 'loss_rpn_box_reg': tensor(0.0108)}\n",
      "torch.Size([3, 313, 500])\n",
      "{'loss_classifier': tensor(0.2797), 'loss_box_reg': tensor(0.0743), 'loss_mask': tensor(0.6720), 'loss_objectness': tensor(0.0530), 'loss_rpn_box_reg': tensor(0.0118)}\n",
      "torch.Size([3, 768, 768])\n",
      "{'loss_classifier': tensor(0.7062), 'loss_box_reg': tensor(0.2562), 'loss_mask': tensor(0.7382), 'loss_objectness': tensor(0.0541), 'loss_rpn_box_reg': tensor(0.0182)}\n",
      "torch.Size([3, 682, 1024])\n",
      "{'loss_classifier': tensor(0.2669), 'loss_box_reg': tensor(0.1474), 'loss_mask': tensor(0.7948), 'loss_objectness': tensor(0.0295), 'loss_rpn_box_reg': tensor(0.0028)}\n",
      "torch.Size([3, 391, 500])\n",
      "{'loss_classifier': tensor(1.2003), 'loss_box_reg': tensor(0.3223), 'loss_mask': tensor(0.7013), 'loss_objectness': tensor(0.2002), 'loss_rpn_box_reg': tensor(0.0532)}\n",
      "torch.Size([3, 323, 576])\n",
      "{'loss_classifier': tensor(0.1518), 'loss_box_reg': tensor(0.0712), 'loss_mask': tensor(0.5505), 'loss_objectness': tensor(0.0705), 'loss_rpn_box_reg': tensor(0.0027)}\n",
      "torch.Size([3, 1024, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_424\\3389598046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdataset_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSolarDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIMAGE_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotation_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mANNOTATION_JSON_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_424\\2279026137.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataset_train, dataset_val, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mavg_train_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mavg_val_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Save the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_424\\1091351062.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, dataset_val, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[operator]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;31m# RPN uses all feature maps that are available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mobjectness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0manchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mbbox_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mbbox_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\panos\\Anaconda3\\envs\\CV_Env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "if args.mode == \"train\":\n",
    "    config = SolarConfig()\n",
    "    assert args.dataset, \"Argument --dataset is required for training\"\n",
    "    config = SolarConfig()\n",
    "    # Prepare datasets\n",
    "    dataset_train = SolarDataset(dataset_dir=IMAGE_DATA_DIR, annotation_dir=ANNOTATION_JSON_PATH, mode=\"train\", val_size=0.2)\n",
    "    dataset_val = SolarDataset(dataset_dir=IMAGE_DATA_DIR, annotation_dir=ANNOTATION_JSON_PATH, mode=\"val\", val_size=0.2)\n",
    "\n",
    "    train(model, dataset_train, dataset_val, device)\n",
    "    \n",
    "else:\n",
    "    assert args.image, \"Provide --image\"\n",
    "\n",
    "    class InferenceConfig(SolarConfig):\n",
    "        # Set batch size to 1 since we'll be running inference on\n",
    "        # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = 1\n",
    "    config = InferenceConfig()\n",
    "    config.display()\n",
    "        \n",
    "    print(\"Weights: \", args.weights)   # Think if I should add logger instead of print\n",
    "    print(\"Dataset: \", args.dataset)\n",
    "    print(\"Logs: \", args.logs)\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(args.weights)) ##########\n",
    "    model.to(device)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(args.image).convert(\"RGB\")\n",
    "\n",
    "# # Define preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image_tensor = preprocess(image).to(device)  # e.g. transforms.ToTensor()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model([image_tensor])\n",
    "    detect_and_color_splash_pytorch(model, args.image, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
